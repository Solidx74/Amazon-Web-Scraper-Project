{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a976163-f74d-49d1-a2a8-75627df3fe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "import smtplib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c59ac7-9832-453e-a2a5-38a615327999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Data Minded, Funny MIS Data Systems Business Analyst Premium T-Shirt\n",
      "Color: Black\n",
      "Fit Type: Men\n"
     ]
    }
   ],
   "source": [
    "# Connect to Website and pull in data\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://www.amazon.com/Data-MIS-Systems-Business-Analyst/dp/B0DKMXR53N?customId=B07536XX75&customizationToken=MC_Assembly_1%23B07536XX75&th=1&psc=1\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/145.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "page = requests.get(URL, headers=headers)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# TITLE\n",
    "title_tag = soup.find(\"span\", id=\"productTitle\")\n",
    "title = title_tag.get_text(strip=True) if title_tag else \"Title not found\"\n",
    "\n",
    "# COLOR\n",
    "color_tag = soup.find(\"span\", id=\"inline-twister-expanded-dimension-text-color_name\")\n",
    "color = color_tag.get_text(strip=True) if color_tag else \"Color not found\"\n",
    "\n",
    "# FIT TYPE\n",
    "fit_tag = soup.find(\"span\", id=\"inline-twister-expanded-dimension-text-fit_type\")\n",
    "fit_type = fit_tag.get_text(strip=True) if fit_tag else \"Fit type not found\"\n",
    "\n",
    "print(\"Title:\", title)\n",
    "print(\"Color:\", color)\n",
    "print(\"Fit Type:\", fit_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a1fccb-e53f-4498-84e0-01bc0b5b8930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Minded, Funny MIS Data Systems Business Analyst Premium T-Shirt\n",
      "Black\n",
      "Men\n"
     ]
    }
   ],
   "source": [
    "# Clean up the data a little bit\n",
    "title = title.strip()\n",
    "color = color.strip()\n",
    "fit_type = fit_type.strip()\n",
    "\n",
    "print(title)\n",
    "print(color)\n",
    "print(fit_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59cf34ee-415e-4c7d-adc7-8451b6584b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-27\n"
     ]
    }
   ],
   "source": [
    "# Create a Timestamp for your output to track when data was collected\n",
    "\n",
    "import datetime\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "421af5fa-f525-449a-9f7c-654c3a9c1ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create CSV and write headers and data into the file\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.date.today()\n",
    "\n",
    "# Define headers and data (with date)\n",
    "header = ['Title', 'Color', 'Fit Type', 'Date']\n",
    "data = [title, color, fit_type, today]\n",
    "\n",
    "# Write to CSV\n",
    "with open('AmazonWebScraperDataset.csv', 'w', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerow(data)\n",
    "\n",
    "print(\"CSV file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b978ffbf-e0d4-4a85-a46c-63eb4696701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Color Fit Type  \\\n",
      "0  Data Minded, Funny MIS Data Systems Business A...  Black      Men   \n",
      "\n",
      "         Date  \n",
      "0  2026-02-27  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\hp\\AmazonWebScraperDataset.csv')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8392c9-5e9b-45e7-9edc-66d0c51de19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we are appending data to the csv\n",
    "\n",
    "with open('AmazonWebScraperDataset.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7bec301-69aa-417a-a45f-066bfa73f89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data added to CSV successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import csv\n",
    "\n",
    "# Combine all of the above code into one function\n",
    "\n",
    "def check_product():\n",
    "\n",
    "    URL = \"https://www.amazon.com/Data-MIS-Systems-Business-Analyst/dp/B0DKMXR53N?customId=B07536XX75&customizationToken=MC_Assembly_1%23B07536XX75&th=1&psc=1\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/145.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "\n",
    "    page = requests.get(URL, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    # Extract data\n",
    "    title = soup.find(\"span\", id=\"productTitle\").get_text(strip=True)\n",
    "    color = soup.find(\"span\", id=\"inline-twister-expanded-dimension-text-color_name\").get_text(strip=True)\n",
    "    fit_type = soup.find(\"span\", id=\"inline-twister-expanded-dimension-text-fit_type\").get_text(strip=True)\n",
    "\n",
    "    # Get today's date\n",
    "    today = datetime.date.today()\n",
    "\n",
    "    # Clean data (extra safety)\n",
    "    title = \" \".join(title.split())\n",
    "    color = \" \".join(color.split())\n",
    "    fit_type = \" \".join(fit_type.split())\n",
    "\n",
    "    # Append data to CSV\n",
    "    data = [title, color, fit_type, today]\n",
    "\n",
    "    with open('AmazonWebScraperDataset.csv', 'a+', newline='', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "\n",
    "    print(\"Data added to CSV successfully.\")\n",
    "\n",
    "# Call the function\n",
    "check_product()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9307aee-8bff-4735-92dc-7854eedd9892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data added to CSV successfully.\n",
      "Waiting 24 hours before next scrape...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Make sure the check_product() function from before is already defined\n",
    "\n",
    "# Runs check_product after a set time and inputs data into your CSV\n",
    "while True:\n",
    "    check_product()\n",
    "    print(\"Waiting 24 hours before next scrape...\")\n",
    "    time.sleep(86400)  # Sleep for 24 hours (86400 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f496a-cb40-4ff7-ad79-6d148d9e78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\hp\\AmazonWebScraperDataset.csv')\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
